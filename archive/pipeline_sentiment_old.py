"""
IMPORTANT ETHICAL NOTE:
This analysis identifies structural and behavioral signals in communication
networks. Metrics are diagnostic indicators only and should not be used
to label individual performance or intent.
"""

import csv
import re
import networkx as nx
from textblob import TextBlob
from collections import defaultdict
import numpy as np

# ----------------------------------------------------------------------
# 1. ë°ì´í„° ë¡œë“œ ë° ê°ì„±/ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤€ë¹„
# ----------------------------------------------------------------------
def analyze_enron_deep(file_path, limit=3000):
    # ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„ (DiGraph) ìƒì„±
    G = nx.DiGraph()
    
    # ë°ì´í„° ì €ì¥ì†Œ
    user_sentiments = defaultdict(list) # ìœ ì €ê°€ ë³´ë‚¸ ë©”ì¼ë“¤ì˜ ê°ì • ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
    after_hours_count = defaultdict(int) # ì—…ë¬´ ì‹œê°„ ì™¸ ë©”ì¼ ì¹´ìš´íŠ¸
    
    print(f"ğŸ“‚ '{file_path}' ì‹¬í™” ë¶„ì„ ì‹œì‘ (ìµœëŒ€ {limit}ê±´)...")
    
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        reader = csv.reader(f)
        next(reader) # í—¤ë” ê±´ë„ˆë›°ê¸°
        
        count = 0
        for row in reader:
            if count >= limit: break
            
            raw_message = row[1]
            
            # [íŒŒì‹±] ë³´ë‚¸ì‚¬ëŒ, ë°›ëŠ”ì‚¬ëŒ, ë‚ ì§œ, ë³¸ë¬¸ ì¶”ì¶œ
            from_match = re.search(r"From: ([\w\.-]+@[\w\.-]+)", raw_message)
            to_match = re.search(r"To: (.*?)\nSubject:", raw_message, re.DOTALL)
            date_match = re.search(r"Date: .*? (\d{2}):\d{2}:\d{2}", raw_message) # ì‹œê°„ ì¶”ì¶œ
            
            # ë³¸ë¬¸ ì¶”ì¶œ (í—¤ë” ì´í›„ ë‚´ìš©) - ê°ì„± ë¶„ì„ìš©
            body_match = re.search(r"\n\n(.*)", raw_message, re.DOTALL)
            
            if from_match and to_match:
                sender = from_match.group(1).strip()
                receivers = [r.strip() for r in to_match.group(1).split(',') if '@' in r]
                
                # [ê°ì„± ë¶„ì„] -1(ë§¤ìš° ë¶€ì •) ~ +1(ë§¤ìš° ê¸ì •)
                sentiment_score = 0
                if body_match:
                    body_text = body_match.group(1)
                    blob = TextBlob(body_text)
                    sentiment_score = blob.sentiment.polarity
                    user_sentiments[sender].append(sentiment_score)

                # [ì‹œê°„ ë¶„ì„] ì›Œë¼ë°¸ íŒŒê´´ì ì°¾ê¸° (ì˜¤í›„ 7ì‹œ ~ ì˜¤ì „ 6ì‹œ ì‚¬ì´ ë°œì†¡)
                if date_match:
                    hour = int(date_match.group(1))
                    if hour >= 19 or hour < 6:
                        after_hours_count[sender] += 1

                # [ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•]
                for receiver in receivers:
                    receiver = receiver.strip()
                    # ì—£ì§€ì— 'ê°ì • ì ìˆ˜'ë¥¼ ì†ì„±ìœ¼ë¡œ ì¶”ê°€
                    G.add_edge(sender, receiver, sentiment=sentiment_score)
                
                count += 1
                if count % 500 == 0: print(f"  - {count}ê±´ ì²˜ë¦¬ ì¤‘...")

    return G, user_sentiments, after_hours_count

# ----------------------------------------------------------------------
# 2. HR ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ë¡œì§
# ----------------------------------------------------------------------
def generate_hr_insights(G, user_sentiments, after_hours_count):
    print("\n" + "="*60)
    print("ğŸ“‹ HR Analyst ë¦¬ë”ì‹­ & ì¡°ì§ë¬¸í™” ì§„ë‹¨ ë¦¬í¬íŠ¸")
    print("="*60)

    # 1. [Toxic Influencer] ì˜í–¥ë ¥ì€ í°ë°, í‰ê·  ì–¸ì–´ê°€ ë¶€ì •ì ì¸ ì‚¬ëŒ
    # PageRank(ì˜í–¥ë ¥) ê³„ì‚°
    pagerank = nx.pagerank(G)
    
    toxic_candidates = []
    for user, scores in user_sentiments.items():
        if len(scores) > 5: # ìµœì†Œ 5í†µ ì´ìƒ ë³´ë‚¸ ì‚¬ëŒë§Œ
            avg_sentiment = np.mean(scores)
            influence = pagerank.get(user, 0)
            # ì¡°ê±´: ì˜í–¥ë ¥ ìƒìœ„ê¶Œì´ë©´ì„œ, ê°ì •ì´ ë¶€ì •ì (< 0)ì¸ ì‚¬ëŒ
            if avg_sentiment < 0: 
                toxic_candidates.append((user, avg_sentiment, influence))
    
    # ì˜í–¥ë ¥ * ë¶€ì •ì ê°•ë„ ë¡œ ì •ë ¬
    toxic_candidates.sort(key=lambda x: x[1] * x[2]) # (ìŒìˆ˜ * ì–‘ìˆ˜ = ë” ì‘ì€ ìŒìˆ˜ê°€ 1ë“±)
    
    print("\nğŸ¤¬ [Toxic Influencer] ë¶€ì •ì  ì–¸ì–´ë¥¼ ì „íŒŒí•˜ëŠ” ì˜í–¥ë ¥ì (Top 3)")
    if toxic_candidates:
        for i, (user, sent, inf) in enumerate(toxic_candidates[:3]):
            print(f"  {i+1}. {user} (ê°ì •: {sent:.2f}, ì˜í–¥ë ¥: {inf:.4f})")
            print(f"     -> í•´ì„: ì¡°ì§ ë‚´ ë¶€ì •ì  ê¸°ë¥˜ë¥¼ í˜•ì„±í•  ìœ„í—˜ì´ í¼.")
    else:
        print("  - ëšœë ·í•œ Toxic Influencerê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # 2. [Passive Leader / Bottleneck] ì •ë³´ ë³‘ëª© í˜„ìƒ
    # In-Degree(ìˆ˜ì‹ )ëŠ” ë†’ì€ë° Out-Degree(ë°œì‹ )ê°€ ë‚®ì€ ì‚¬ëŒ
    bottlenecks = []
    for user in G.nodes():
        in_d = G.in_degree(user)
        out_d = G.out_degree(user)
        if in_d > 10: # ì¶©ë¶„íˆ ë©”ì¼ì„ ë°›ëŠ” ì‚¬ëŒ ì¤‘ì—ì„œ
            ratio = out_d / (in_d + 1)
            if ratio < 0.1: # ë°›ì€ ê²ƒì— ë¹„í•´ ë³´ë‚¸ ê²Œ 10% ë¯¸ë§Œ
                bottlenecks.append((user, in_d, out_d))
    
    bottlenecks.sort(key=lambda x: x[1], reverse=True)
    
    print("\nğŸ•³ï¸ [Passive Leader] ì •ë³´ ë¸”ë™í™€/ë³‘ëª© ì˜ì‹¬ì (Top 3)")
    for i, (user, in_d, out_d) in enumerate(bottlenecks[:3]):
        print(f"  {i+1}. {user} (ìˆ˜ì‹ : {in_d}, ë°œì‹ : {out_d})")
        print(f"     -> í•´ì„: ì˜ì‚¬ê²°ì •ì´ ì§€ì—°ë˜ê±°ë‚˜, íŒ€ì›ë“¤ì´ ë‹µë‹µí•´í•  ìˆ˜ ìˆìŒ.")

    # 3. [Misbehavior] ì›Œë¼ë°¸ íŒŒê´´ì (After-hours emailing)
    sorted_workaholics = sorted(after_hours_count.items(), key=lambda x: x[1], reverse=True)
    print("\nğŸŒ™ [Misbehavior] ì—…ë¬´ ì‹œê°„ ì™¸ ì´ë©”ì¼ ê³¼ë‹¤ ë°œì†¡ì (Top 3)")
    for i, (user, count) in enumerate(sorted_workaholics[:3]):
        print(f"  {i+1}. {user} ({count}ê±´ ë°œì†¡)")
        print(f"     -> í•´ì„: ë³¸ì¸ì˜ ë²ˆì•„ì›ƒ ìœ„í—˜ + íŒ€ì›ì—ê²Œ ìƒì‹œ ì—°ê²° ì••ë°•ì„ ì¤„ ìˆ˜ ìˆìŒ.")

    # 4. [Silo] ì»¤ë®¤ë‹ˆí‹° ê°ì§€ (ê°„ë‹¨ ë²„ì „)
    # ì—°ê²°ì´ ëŠì–´ì§„ ë…ë¦½ëœ ê·¸ë£¹(Component)ì´ ìˆëŠ”ì§€ í™•ì¸
    components = list(nx.weakly_connected_components(G))
    print(f"\nğŸ§© [Silo ì§„ë‹¨] ì¡°ì§ íŒŒí¸í™” ì •ë„")
    print(f"  - ì „ì²´ ë„¤íŠ¸ì›Œí¬ê°€ {len(components)}ê°œì˜ ë…ë¦½ëœ ê·¸ë£¹ìœ¼ë¡œ ìª¼ê°œì ¸ ìˆìŠµë‹ˆë‹¤.")
    if len(components) > 1:
        print(f"     -> í•´ì„: {len(components)}ê°œì˜ ê·¸ë£¹ì´ ì„œë¡œ ì†Œí†µí•˜ì§€ ì•Šê³  ë‹¨ì ˆë˜ì–´ ìˆìŠµë‹ˆë‹¤(Silo).")
        print(f"     -> ê°€ì¥ í° ê·¸ë£¹ í¬ê¸°: {len(components[0])}ëª…, ë‘ ë²ˆì§¸ ê·¸ë£¹: {len(components[1])}ëª…")
    else:
        print("     -> í•´ì„: ì „ì²´ ì¡°ì§ì´ í•˜ë‚˜ë¡œ ì˜ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# ----------------------------------------------------------------------
# 3. ì‹¤í–‰
# ----------------------------------------------------------------------
if __name__ == "__main__":
    file_path = "data/emails.csv" # ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼ ê²½ë¡œ
    try:
        G, user_sentiments, after_hours_count = analyze_enron_deep(file_path, limit=5000)
        generate_hr_insights(G, user_sentiments, after_hours_count)
    except FileNotFoundError:
        print("íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'emails.csv' ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
